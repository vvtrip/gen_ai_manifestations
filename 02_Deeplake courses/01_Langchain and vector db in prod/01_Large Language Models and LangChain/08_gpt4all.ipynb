{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90076622-e270-4b81-9a76-0c5c4bf4ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How GPT4All works?\n",
    "# It is trained on top of Facebookâ€™s LLaMA model, which released its weights under a non-commercial license. \n",
    "# Still, running the mentioned architecture on your local PC is impossible due to the large (7 billion) number of parameters.\n",
    "# The authors incorporated two tricks to do efficient fine-tuning and inference. \n",
    "# We will focus on inference since the fine-tuning process is out of the scope of this course.\n",
    "\n",
    "# The main contribution of GPT4All models is the ability to run them on a CPU. Testing these models is practically free \n",
    "# because the recent PCs have powerful Central Processing Units. The underlying algorithm that helps with making it happen is called Quantization. \n",
    "# It basically converts the pre-trained model weights to 4-bit precision using the GGML format.  \n",
    "# So, the model uses fewer bits to represent the numbers. There are two main advantages to using this technique:\n",
    "\n",
    "# Reducing Memory Usage: It makes deploying the models more efficient on low-resource devices.\n",
    "# Faster Inference: The models will be faster during the generation process since there will be fewer computations.\n",
    "# It is true that we are sacrificing quality by a small margin when using this approach. However, \n",
    "# it is a trade-off between no access at all and accessing a slightly underpowered model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907dc630-5974-40c6-8f24-41001fe6e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d314ee-a19a-4e3b-9b3d-96ede4e8a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9beb409-fe86-4bce-b6b4-2db6c815f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert the downloaded gpt4all-lora-quantized-ggml.bin model into latest format\n",
    "# git clone https://github.com/ggerganov/llama.cpp.git\n",
    "# cd llama.cpp && git checkout 2b26469\n",
    "# python3 llama.cpp/convert.py ./models/gpt4all-lora-quantized-ggml.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6777e5-ea1f-42a8-b48c-db95198eff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following code would run with these versions of langchain and pyllamacpp\n",
    "# pip install -q langchain==0.0.152 pyllamacpp==1.0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48656d54-fa2d-41a6-96ee-aab54bb92cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = GPT4All(model=\"./ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523cf7ae-5bec-4d16-a57d-9c73dda29142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: What happens when it rains somewhere?\n",
      "\n",
      "Answer: Let's think step by step. When there is rain, water vapors are released into the atmosphere from various sources such as evaporated sweat and respiratory exhalations of humans or animals; these forms can lead to cloud formation which then proceed with precipitation. As a result, raindrops form on clouds that appear in different sizes depending upon their altitude and temperature above them (this is called the wet-bulb thermometer); eventually this rain falls back into earth's surface either through evaporation or wind circulation from weather systems; it then flows down to lakes, rivers, oceans etc. where they form streams & waves which can create floods depending upon their strength and amount of rainfall occurring in a particular area."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Question: What happens when it rains somewhere?\\n\\nAnswer: Let's think step by step. When there is rain, water vapors are released into the atmosphere from various sources such as evaporated sweat and respiratory exhalations of humans or animals; these forms can lead to cloud formation which then proceed with precipitation. As a result, raindrops form on clouds that appear in different sizes depending upon their altitude and temperature above them (this is called the wet-bulb thermometer); eventually this rain falls back into earth's surface either through evaporation or wind circulation from weather systems; it then flows down to lakes, rivers, oceans etc. where they form streams & waves which can create floods depending upon their strength and amount of rainfall occurring in a particular area.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535c13b8-4c20-46b8-bfb4-533da743e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default behavior is to wait for the model to finish its inference process to print out its outputs. \n",
    "# However, it could take more than an hour (depending on your hardware) to \n",
    "# respond to one prompt because of the large number of parameters in the model. We can use the StreamingStdOutCallbackHandler() \n",
    "# callback to instantly show the latest generated token. This way, we can be sure that the generation process is running and \n",
    "# the model shows the expected behavior. Otherwise, it is possible to stop the inference and adjust the prompt.\n",
    "\n",
    "# The GPT4All class is responsible for reading and initializing the weights file and setting the required callbacks. \n",
    "# Then, we can tie the language model and the prompt using the LLMChain class. It will enable us to ask questions from the model using the run() object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9dbb03-f296-4456-8f06-9fc2a2145793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

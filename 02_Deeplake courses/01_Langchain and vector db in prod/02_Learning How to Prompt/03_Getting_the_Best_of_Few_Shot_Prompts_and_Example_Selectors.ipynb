{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9707820-b207-4475-918e-13e7aed90284",
   "metadata": {},
   "source": [
    "# Alternating Human/AI messages for CHAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb380c-5c3b-4edd-b171-c58ef2ed8a39",
   "metadata": {},
   "source": [
    "In this strategy, few-shot prompting utilizes alternating human and AI messages. This technique can be especially beneficial for chat-oriented applications since the language model must comprehend the conversational context and provide appropriate responses.\n",
    "\n",
    "While this approach effectively handles conversation context and is easy to implement for chat-based applications, it lacks flexibility for other application types and is limited to chat-based models. However, we can use alternating human/AI messages to create a chat prompt that translates English into pirate language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c88c69-7aa6-4358-8ccf-451eaa67f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2959e8-b781-4729-99c1-56cb805b74d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv('newenv.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc62d90-adb2-4139-9ef2-2f242559355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaibhav\\AppData\\Local\\Temp\\ipykernel_22680\\895470827.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94ca91d2-fd09-4464-ad70-5c02560abf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "\n",
    "system_template = \"You are a helpful assisant for converting english queries into pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt  = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6608a3ec-85b9-4671-b6c6-7d758f1ebc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I be lovin' the art o' codin', aye!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "chain = LLMChain(llm=llm, prompt = chat_prompt)\n",
    "chain.run(\"I love programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbb4539-afdb-4844-a32c-cfbfc31cccfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hi'), additional_kwargs={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb336f9f-fb99-4001-9e42-68b8375e0c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Argh me mateys'), additional_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de08ca8d-a165-4a8e-9ae7-1f21223626c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assisant for converting english queries into pirate.'), additional_kwargs={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "350348ae-79b0-49af-ae17-5f455288548f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95840f57-4c01-468f-b475-6a81fc62b381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assisant for converting english queries into pirate.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hi'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Argh me mateys'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ecbb43-ca96-4774-8993-3b7e622f1ceb",
   "metadata": {},
   "source": [
    "# Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2330f84-587a-4926-becb-7ef56da63167",
   "metadata": {},
   "source": [
    "Few-shot prompting can lead to improved output quality because the model can learn the task better by observing the examples. However, the increased token usage may worsen the results if the examples are not well chosen or are misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd137085-0e43-41af-aca2-061e96bc259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\":\"\",\n",
    "        \"answer\":\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"\",\n",
    "        \"answer\":\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"\",\n",
    "        \"answer\":\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"\",\n",
    "        \"answer\":\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables = [\"query\",\"answer\"],\n",
    "    template = example_template\n",
    "    )\n",
    "\n",
    "prefix = \"\"\"\n",
    "The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "examples = examples,\n",
    "example_prompt = example_prompt,\n",
    "prefix = prefix,\n",
    "suffix= suffix,\n",
    "input_variables = [\"query\"],\n",
    "example_separator = \"\\n\\n\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc9da68c-66ca-4d53-9ec7-3f55116417d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The secret to happiness? Well, it’s a mix of chocolate, a good Wi-Fi connection, and the occasional dance party in your living room. Just remember, if you can’t find happiness, try looking under the couch cushions—sometimes it’s hiding with the remote!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the secret to happiness?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb2eb8-6f5a-46a7-85de-378c96d61aa9",
   "metadata": {},
   "source": [
    "# Example Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259966e1-a466-44d4-9450-7fd2523b21c7",
   "metadata": {},
   "source": [
    "Example selectors can be used to provide a few-shot learning experience. The primary goal of few-shot learning is to learn a similarity function that maps the similarities between classes in the support and query sets. In this context, an example selector can be designed to choose a set of relevant examples that are representative of the desired output.\n",
    "\n",
    "The **LengthBasedExampleSelector** is useful when you're concerned about the length of the context window. It selects fewer examples for longer queries and more examples for shorter queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9dfdec4-0871-488e-97ec-91ab422fa6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8be2d48-4e6e-496b-ba99-dddea993e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
    "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
    "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_template # or template = \"\"\"Word: {word}\\nAntonym: {antonym}\"\"\"\n",
    ")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fa45d5-68a0-405e-8c35-9ce03bf002dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Word: {word}\\nAntonym:\",\n",
    "    input_variables=[\"word\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6629a47-9c16-4393-b47c-841ed526cf3e",
   "metadata": {},
   "source": [
    "**We can generate a prompt using the format method for understanding intermediate functionality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b60cdddc-ebcd-4056-917e-ade04bb9957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "\n",
      "\n",
      "Word: energetic\n",
      "Antonym: lethargic\n",
      "\n",
      "\n",
      "\n",
      "Word: sunny\n",
      "Antonym: gloomy\n",
      "\n",
      "\n",
      "Word: big\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt.format(word=\"big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19120a4b-ff18-413a-9c51-68747a70a21e",
   "metadata": {},
   "source": [
    "This method is effective for managing a large number of examples. It offers customization through various selectors, but it involves manual creation and selection of examples, which might not be ideal for every application type.\n",
    "\n",
    "Example of employing LangChain's **SemanticSimilarityExampleSelector** for selecting examples based on their semantic resemblance to the input. This illustration showcases the process of creating an ExampleSelector, generating a prompt using a few-shot approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9d15cfb-598d-411a-99bb-d66dd544e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eff6b2b6-8d6f-4c6a-a6ac-89faa56141d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Define some examples\n",
    "examples = [\n",
    "    {\"input\": \"0°C\", \"output\": \"32°F\"},\n",
    "    {\"input\": \"10°C\", \"output\": \"50°F\"},\n",
    "    {\"input\": \"20°C\", \"output\": \"68°F\"},\n",
    "    {\"input\": \"30°C\", \"output\": \"86°F\"},\n",
    "    {\"input\": \"40°C\", \"output\": \"104°F\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a1fe224-e2aa-4596-80ab-4fe1c90bcfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# create Deep Lake dataset locally\n",
    "db = DeepLake(dataset_path='./mydb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82119e4d-09ac-4e7a-adce-414fc12d9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings, OpenAIEmbeddings\n",
    "\n",
    "# hf = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en\", model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True})\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00a5141f-c2c5-46e0-b632-5445f0292a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 5 embeddings in 1 batches of size 5:: 100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (5, 1)      str     None   \n",
      " metadata     json      (5, 1)      str     None   \n",
      " embedding  embedding  (5, 1536)  float32   None   \n",
      "    id        text      (5, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, embeddings, db, k=1\n",
    ")\n",
    "\n",
    "# Create a FewShotPromptTemplate using the example_selector\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
    "    suffix=\"Input: {temperature}\\nOutput:\", \n",
    "    input_variables=[\"temperature\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f21400ac-d520-464e-9f7c-06955550acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 10°C\n",
      "Output: 50°F\n",
      "\n",
      "Input: 10°C\n",
      "Output:\n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 30°C\n",
      "Output: 86°F\n",
      "\n",
      "Input: 30°C\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Test the similar_prompt with different inputs\n",
    "print(similar_prompt.format(temperature=\"10°C\"))   # Test with an input\n",
    "print(similar_prompt.format(temperature=\"30°C\"))  # Test with another input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff9ab697-e45b-4b79-9063-e266bb44b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 1 embeddings in 1 batches of size 1:: 100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (6, 1)      str     None   \n",
      " metadata     json      (6, 1)      str     None   \n",
      " embedding  embedding  (6, 1536)  float32   None   \n",
      "    id        text      (6, 1)      str     None   \n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 40°C\n",
      "Output: 104°F\n",
      "\n",
      "Input: 40°C\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Add a new example to the SemanticSimilarityExampleSelector\n",
    "similar_prompt.example_selector.add_example({\"input\": \"50°C\", \"output\": \"122°F\"})\n",
    "print(similar_prompt.format(temperature=\"40°C\")) # Test with a new input after adding the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb14aeb-5adf-46e6-8412-b3a9e666d7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

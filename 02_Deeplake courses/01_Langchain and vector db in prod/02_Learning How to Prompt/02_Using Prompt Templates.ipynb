{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7693d1-2efd-4076-848e-b04b5aa9db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "import openai, langchain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3fe21d-ee3c-45f1-bf59-bf0d47b97faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv('newenv.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d88647f-779f-4184-a55b-46c97eab7e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.13'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f276ca-2f73-44fa-a99d-6f6225904e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.40.6'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d939047b-b6b6-43fd-9828-517daeebfc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of code, where logic takes flight,  \n",
      "Dwells a concept, wondrous and slight.  \n",
      "Recursion, the dance of a function so bold,  \n",
      "A tale of itself, repeatedly retold.  \n",
      "\n",
      "Picture a mirror, reflecting what's there,  \n",
      "Each glance at itself, a familiar affair.  \n",
      "A function calls forth, its own name in the chase,  \n",
      "To solve a small puzzle, it takes a new face.  \n",
      "\n",
      "Like nesting dolls in a vibrant embrace,  \n",
      "Each one holds a story, a smaller trace.  \n",
      "“Until a base case,” the wise coders say,  \n",
      "For without it, our echoes will lead us astray.  \n",
      "\n",
      "Down the rabbit hole, deeper we dive,  \n",
      "With each invocation, we strive to survive.  \n",
      "Break down the problem, don’t fear the descent,  \n",
      "For every small step leads closer, we’re meant.  \n",
      "\n",
      "A factorial’s charm, it shows what we seek,  \n",
      "Five times four times three, down to just one, unique.  \n",
      "So: five factorial calls four, who calls three,  \n",
      "Each layer unfolds, till one’s left to see.  \n",
      "\n",
      "Oh, cherish the beauty of recursion’s embrace,  \n",
      "Where functions reflect in a cyclical space.  \n",
      "But beware, dear coder, lest you lose your way,  \n",
      "For too many reflections can lead to dismay.  \n",
      "\n",
      "So keep your base case, your anchor so true,  \n",
      "With each self-invocation, let clarity shine through.  \n",
      "In the art of recursion, there’s elegance found,  \n",
      "In each looping whisper, your logic unbound.  \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a6d74b-63b8-420c-8c05-5aaf70c457fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of October 2023, the top five highest-grossing movies of all time by worldwide box office revenue are:\n",
      "\n",
      "1. **Avatar** (2009) - Directed by James Cameron, this sci-fi epic has grossed over $2.9 billion.\n",
      "2. **Avengers: Endgame** (2019) - Directed by Anthony and Joe Russo, this Marvel superhero film has grossed approximately $2.798 billion.\n",
      "3. **Titanic** (1997) - Also directed by James Cameron, this romantic drama has grossed around $2.2 billion.\n",
      "4. **Star Wars: The Force Awakens** (2015) - Directed by J.J. Abrams, this installment of the Star Wars saga has grossed about $2.068 billion.\n",
      "5. **Avengers: Infinity War** (2018) - Directed by Anthony and Joe Russo, this film has grossed around $2.048 billion.\n",
      "\n",
      "Please note that box office figures are constantly changing due to re-releases and new films, so it's always good to check for the most current information.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "messages= [\n",
    "    (\"system\", \"You are an expert assistant about movies.\"),\n",
    "    (\"human\", \"Can you tell top 5 movies of all time by revenue?\")\n",
    "]\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fbcfb0a-c74a-47e6-973f-1715f0f9a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import OpenAI\n",
    "# llm = OpenAI(model = \"gpt-3.5-turbo-instruct\", temperature=0) #gpt-4o-mini, gpt-3.5-turbo, gpt-4o are chat models so use ChatOpenAI for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f320a8-ddb8-44c0-97fb-80340838705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main advantage of quantum computing over classical computing?\n",
      "Answer: The main advantage of quantum computing over classical computing is its ability to solve complex problems faster.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided, answer\n",
    "with \"I don't know\".\n",
    "Context: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n",
    "...\n",
    "Question: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Set the query you want to ask\n",
    "input_data = {\"query\": \"What is the main advantage of quantum computing over classical computing?\"}\n",
    "\n",
    "# Run the LLMChain to get the AI-generated answer\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Question:\", input_data[\"query\"])\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2372b6e-9d95-4715-a157-4ea88a8442c5",
   "metadata": {},
   "source": [
    "To create a PromptTemplate object, two arguments are required:\n",
    "\n",
    "- input_variables: A list of variable names in the template; in this case, it includes only the query.\n",
    "- template: The template string containing formatted text and placeholders.\n",
    "\n",
    "After creating the PromptTemplate object, it can be used to produce prompts with specific questions by providing input data. The input data is a dictionary where the key corresponds to the variable name in the template. The resulting prompt can then be passed to a language model to generate answers.- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e30783-36c7-4676-b07e-1e142d136616",
   "metadata": {},
   "source": [
    "# FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "016fa443-8bd8-4f12-ac67-fa31fa1cc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232c9afb-724c-4823-89ab-8b2085d4fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal: tiger  \n",
      "Habitat: tropical rainforests, grasslands, and mangrove swamps\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\"animal\": \"lion\", \"habitat\": \"savanna\"},\n",
    "    {\"animal\": \"polar bear\", \"habitat\": \"Arctic ice\"},\n",
    "    {\"animal\": \"elephant\", \"habitat\": \"African grasslands\"}\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Animal: {animal}\n",
    "Habitat: {habitat}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"animal\", \"habitat\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Identify the habitat of the given animal\",\n",
    "    suffix=\"Animal: {input}\\nHabitat:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
    "\n",
    "input_data = {\"input\": \"tiger\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3442377-b9ea-4b0e-a10c-ae5f63854bd1",
   "metadata": {},
   "source": [
    "Additionally, you can also save your PromptTemplate to a file in your local filesystem in JSON or YAML format:\n",
    "\n",
    "     prompt_template.save(\"awesome_prompt.json\")\n",
    "\n",
    "and load it back \n",
    "\n",
    "    from langchain.prompts import load_prompt\n",
    "    loaded_prompt = load_prompt(\"awesome_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26dfe111-2803-48c9-a489-6bfec0a3f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start by mastering the art of confusing your friends at parties. Then, dive into some textbooks that make you question your sanity!\n"
     ]
    }
   ],
   "source": [
    "#sarcasm example\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I become a better programmer?\",\n",
    "        \"answer\": \"Try talking to a rubber duck; it works wonders.\"\n",
    "    }, {\n",
    "        \"query\": \"Why is the sky blue?\",\n",
    "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative and funny responses to users' questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"How can I learn quantum computing?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae89b87-8717-4675-b555-c18fab671621",
   "metadata": {},
   "source": [
    "Instead of utilizing the examples list of dictionaries directly, we can implement a **LengthBasedExampleSelector** like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcbd4bc8-abc1-4cc8-b5a7-710d5229376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do you feel today?\",\n",
    "        \"answer\": \"As an AI, I don't have feelings, but I've got jokes!\"\n",
    "    }, {\n",
    "        \"query\": \"What is the speed of light?\",\n",
    "        \"answer\": \"Fast enough to make a round trip around Earth 7.5 times in one second!\"\n",
    "    }, {\n",
    "        \"query\": \"What is a quantum computer?\",\n",
    "        \"answer\": \"A magical box that harnesses the power of subatomic particles to solve complex problems.\"\n",
    "    }, {\n",
    "        \"query\": \"Who invented the telephone?\",\n",
    "        \"answer\": \"Alexander Graham Bell, the original 'ringmaster'.\"\n",
    "    }, {\n",
    "        \"query\": \"What programming language is best for AI development?\",\n",
    "        \"answer\": \"Python, because it's the only snake that won't bite.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris, the city of love and baguettes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is photosynthesis?\",\n",
    "        \"answer\": \"A plant's way of saying 'I'll turn this sunlight into food. You're welcome, Earth.'\"\n",
    "    }, {\n",
    "        \"query\": \"What is the tallest mountain on Earth?\",\n",
    "        \"answer\": \"Mount Everest, Earth's most impressive bump.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the most abundant element in the universe?\",\n",
    "        \"answer\": \"Hydrogen, the basic building block of cosmic smoothies.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the largest mammal on Earth?\",\n",
    "        \"answer\": \"The blue whale, the original heavyweight champion of the world.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the fastest land animal?\",\n",
    "        \"answer\": \"The cheetah, the ultimate sprinter of the animal kingdom.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the square root of 144?\",\n",
    "        \"answer\": \"12, the number of eggs you need for a really big omelette.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the average temperature on Mars?\",\n",
    "        \"answer\": \"Cold enough to make a Martian wish for a sweater and a hot cocoa.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9358d9d5-4e16-47c7-b684-94fbbdb86e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1697bbf3-4507-4bb0-b2aa-e741e8d7bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60dcba-c7d1-40a7-ab61-9583782e0365",
   "metadata": {},
   "source": [
    "So, the dynamic_prompt_template utilizes the example_selector instead of a fixed list of examples. This allows the FewShotPromptTemplate to adjust the number of included examples based on the length of the input query. By doing so, it optimizes the use of the available context window and ensures that the language model receives an appropriate amount of contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b0b8fe-3f19-407d-a518-dbfedd94dd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander Graham Bell, the guy who really knew how to make a call!\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"Who invented the telephone?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02cd912-0fd4-471a-9157-9a6f22249c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

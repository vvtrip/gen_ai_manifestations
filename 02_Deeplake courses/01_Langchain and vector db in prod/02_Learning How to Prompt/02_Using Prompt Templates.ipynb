{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b7693d1-2efd-4076-848e-b04b5aa9db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "import openai, langchain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe3fe21d-ee3c-45f1-bf59-bf0d47b97faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv('newenv.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d88647f-779f-4184-a55b-46c97eab7e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.13'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23f276ca-2f73-44fa-a99d-6f6225904e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.40.6'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d939047b-b6b6-43fd-9828-517daeebfc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the land of code where logic reigns,  \n",
      "Where loops abound and flow charts wane,  \n",
      "There's a tale of a curious function,  \n",
      "A magical trick called recursion.\n",
      "\n",
      "Picture a mirror in a hall so wide,  \n",
      "Reflecting reflections, on either side.  \n",
      "A call to the self, a loop so divine,  \n",
      "Where each answer awaits, as it starts to unwind.\n",
      "\n",
      "“A problem, oh problem!” begins with despair,  \n",
      "But the recursive function whispers with flair,  \n",
      "“Break it down gently, into smaller parts,  \n",
      "For within each piece, the answer imparts.”\n",
      "\n",
      "First, check for a base case, steady and sure,  \n",
      "Without it, dear coder, you’ll find no cure.  \n",
      "It’s the ending condition, the stop signal’s shout,  \n",
      "A lighthouse in fog, guiding you out.\n",
      "\n",
      "Then comes the call, like a ripple on streams,  \n",
      "“Here’s a smaller version,” the function redeems.  \n",
      "It plunges through layers, like diving in deep,  \n",
      "With each recursive call, you take a new leap.\n",
      "\n",
      "As the layers stack up, you climb to the peak,  \n",
      "Where solutions lie waiting, not far, just a tweak.  \n",
      "With a final return, like a boomerang flight,  \n",
      "The answers cascade, like stars in the night.\n",
      "\n",
      "Back through the calls, they gather with grace,  \n",
      "Each returning with wisdom, filling the space.  \n",
      "From chaos to clarity, the puzzle is spun,  \n",
      "Recursion has woven its delicate run.\n",
      "\n",
      "So cherish the dance of this self-same call,  \n",
      "In the realm of your code, it conquers them all.  \n",
      "For in every recursion, a story takes flight,  \n",
      "Of how problem and solution can twirl into light.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a6d74b-63b8-420c-8c05-5aaf70c457fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of October 2023, the top five highest-grossing movies of all time by worldwide box office revenue are:\n",
      "\n",
      "1. **Avatar** (2009) - Directed by James Cameron, this sci-fi epic has grossed over $2.9 billion.\n",
      "2. **Avengers: Endgame** (2019) - Directed by Anthony and Joe Russo, this Marvel superhero film has grossed approximately $2.798 billion.\n",
      "3. **Titanic** (1997) - Also directed by James Cameron, this romantic drama has grossed around $2.2 billion.\n",
      "4. **Star Wars: The Force Awakens** (2015) - Directed by J.J. Abrams, this installment of the Star Wars saga has grossed about $2.068 billion.\n",
      "5. **Avengers: Infinity War** (2018) - Directed by Anthony and Joe Russo, this film has grossed around $2.048 billion.\n",
      "\n",
      "Please note that box office figures are constantly changing due to re-releases and new films, so it's always good to check for the most current data.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "messages= [\n",
    "    (\"system\", \"You are an expert assistant about movies.\"),\n",
    "    (\"human\", \"Can you tell top 5 movies of all time by revenue?\")\n",
    "]\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fbcfb0a-c74a-47e6-973f-1715f0f9a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import OpenAI\n",
    "# llm = OpenAI(model = \"gpt-3.5-turbo-instruct\", temperature=0) #gpt-4o-mini, gpt-3.5-turbo, gpt-4o are chat models so use ChatOpenAI for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5f320a8-ddb8-44c0-97fb-80340838705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main advantage of quantum computing over classical computing?\n",
      "Answer: The main advantage of quantum computing over classical computing is its ability to solve complex problems faster due to its use of quantum mechanics.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided, answer\n",
    "with \"I don't know\".\n",
    "Context: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n",
    "...\n",
    "Question: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Set the query you want to ask\n",
    "input_data = {\"query\": \"What is the main advantage of quantum computing over classical computing?\"}\n",
    "\n",
    "# Run the LLMChain to get the AI-generated answer\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Question:\", input_data[\"query\"])\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2372b6e-9d95-4715-a157-4ea88a8442c5",
   "metadata": {},
   "source": [
    "To create a PromptTemplate object, two arguments are required:\n",
    "\n",
    "- input_variables: A list of variable names in the template; in this case, it includes only the query.\n",
    "- template: The template string containing formatted text and placeholders.\n",
    "\n",
    "After creating the PromptTemplate object, it can be used to produce prompts with specific questions by providing input data. The input data is a dictionary where the key corresponds to the variable name in the template. The resulting prompt can then be passed to a language model to generate answers.- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e30783-36c7-4676-b07e-1e142d136616",
   "metadata": {},
   "source": [
    "# FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "016fa443-8bd8-4f12-ac67-fa31fa1cc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "232c9afb-724c-4823-89ab-8b2085d4fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal: tiger  \n",
      "Habitat: tropical rainforests, grasslands, and mangrove swamps\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\"animal\": \"lion\", \"habitat\": \"savanna\"},\n",
    "    {\"animal\": \"polar bear\", \"habitat\": \"Arctic ice\"},\n",
    "    {\"animal\": \"elephant\", \"habitat\": \"African grasslands\"}\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Animal: {animal}\n",
    "Habitat: {habitat}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"animal\", \"habitat\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Identify the habitat of the given animal\",\n",
    "    suffix=\"Animal: {input}\\nHabitat:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
    "\n",
    "input_data = {\"input\": \"tiger\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3442377-b9ea-4b0e-a10c-ae5f63854bd1",
   "metadata": {},
   "source": [
    "Additionally, you can also save your PromptTemplate to a file in your local filesystem in JSON or YAML format:\n",
    "\n",
    "     prompt_template.save(\"awesome_prompt.json\")\n",
    "\n",
    "and load it back \n",
    "\n",
    "    from langchain.prompts import load_prompt\n",
    "    loaded_prompt = load_prompt(\"awesome_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26dfe111-2803-48c9-a489-6bfec0a3f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start by mastering the art of confusing your friends with Schrödinger jokes. Then, dive into some textbooks that make you question your sanity!\n"
     ]
    }
   ],
   "source": [
    "#sarcasm example\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I become a better programmer?\",\n",
    "        \"answer\": \"Try talking to a rubber duck; it works wonders.\"\n",
    "    }, {\n",
    "        \"query\": \"Why is the sky blue?\",\n",
    "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative and funny responses to users' questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"How can I learn quantum computing?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd4bc8-abc1-4cc8-b5a7-710d5229376c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358d9d5-4e16-47c7-b684-94fbbdb86e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

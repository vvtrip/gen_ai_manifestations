{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae33dd83-297d-443f-a08f-1c92df119c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b818746b-0734-439b-8488-cbbd4a15398e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv('file.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0c8b92-788f-46c5-9ae3-ea068e0124d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-> 0.2.8 openai-> 1.35.14\n"
     ]
    }
   ],
   "source": [
    "import openai,langchain\n",
    "print(\"langchain->\",langchain.__version__, \"openai->\",openai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec892d-cd08-4ec3-964d-3ab980af33e5",
   "metadata": {},
   "source": [
    "# Role  Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac664de-deba-4206-9bd9-706d3959557d",
   "metadata": {},
   "source": [
    "Role prompting involves asking the LLM to assume a specific role or identity before performing a given task, such as acting as a copywriter. This can help guide the model's response by providing a context or perspective for the task. To work with role prompts, you could iteratively:\n",
    "\n",
    "1- Specify the role in your prompt, e.g., \"As a copywriter, create some attention-grabbing taglines for AWS services.\"\n",
    "\n",
    "2 - Use the prompt to generate an output from an LLM.\n",
    "\n",
    "3 - Analyze the generated response and, if necessary, refine the prompt for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b088b4-0caf-459c-919f-64ed6f5ea4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: interstellar travel\n",
      "Year: 3030\n",
      "AI-generated song title: \n",
      "\"Galactic Odyssey: Journey to 3030\"\n"
     ]
    }
   ],
   "source": [
    "# In this example, the LLM is asked to act as a futuristic robot band \n",
    "# conductor and suggest a song title related to the given theme and year\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model = \"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "template = \"\"\"\"\n",
    "            As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
    "            What's a cool song title for a song about {theme} in the year {year}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"theme\", \"year\"],\n",
    "    template = template)\n",
    "\n",
    "input_data = {\"theme\":\"interstellar travel\",\"year\":\"3030\"}\n",
    "\n",
    "chain = LLMChain(llm = llm, prompt=prompt)\n",
    "\n",
    "response = chain.invoke(input_data)\n",
    "\n",
    "print(\"Theme: interstellar travel\")\n",
    "print(\"Year: 3030\")\n",
    "print(\"AI-generated song title:\", response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b377d40-48d1-4620-a731-0c10ec428404",
   "metadata": {},
   "source": [
    "This is a good prompt for several reasons:\n",
    "\n",
    "- **Clear instructions:** The prompt is phrased as a clear request for help in generating a song title, and it specifies the context: \"As a futuristic robot band conductor.\" This helps the LLM understand that the desired output should be a song title related to a futuristic scenario.\n",
    "\n",
    "- **Specificity:** The prompt asks for a song title that relates to a specific theme and a specific year, \"{theme} in the year {year}.\" This provides enough context for the LLM to generate a relevant and creative output. The prompt can be adapted for different themes and years by using input variables, making it versatile and reusable.\n",
    "\n",
    "- **Open-ended creativity:** The prompt allows for open-ended creativity, as it doesn't limit the LLM to a particular format or style for the song title. The LLM can generate a diverse range of song titles based on the given theme and year.\n",
    "Focus on the task: The prompt is focused solely on generating a song title, making it easier for the LLM to provide a suitable output without getting sidetracked by unrelated topics.\n",
    "\n",
    "These elements help the LLM understand the user's intention and generate a suitable response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea73d1c-596d-4327-9fb8-a3e69ade989d",
   "metadata": {},
   "source": [
    "# Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "becf2f8f-9637-49f3-8fc3-39547d2c8c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colour purple\n",
      "Emotion  royalty\n"
     ]
    }
   ],
   "source": [
    "# Few Shot Prompting In the next example, the LLM is asked to provide the emotion \n",
    "# associated with a given color based on a few examples of color-emotion pairs:\n",
    "from langchain import FewShotPromptTemplate\n",
    "examples = [\n",
    "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
    "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
    "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"\n",
    "Color: {color}\n",
    "Emotion: {emotion}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"color\", \"emotion\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix = \"Here are some examples of colors and the emotions associated with them:\\n\\n\",\n",
    "    suffix=\"\\n\\nNow, given a new color, identify the emotion associated with it:\\n\\nColor: {input}\\nEmotion:\",\n",
    "    input_variables=  [\"input\"],\n",
    "    example_separator = \"\\n\")\n",
    "\n",
    "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
    "\n",
    "# The formatted prompt is not passed directly to the chain but type casted as a PromptTemplate object\n",
    "chain = LLMChain(llm = llm, prompt=PromptTemplate(template=formatted_prompt))\n",
    "\n",
    "response  = chain.invoke({})\n",
    "\n",
    "print(\"Colour\",\"purple\")\n",
    "print(\"Emotion\",response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea797eb-26f9-49bb-8dc1-1e68de6e1287",
   "metadata": {},
   "source": [
    "# Bad Prompt practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0955cb35-5f00-4e04-8784-db6a00eb4828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me something about dogs.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  a too-vague prompt that provides little context or guidance for the model to generate a meaningful response.\n",
    "\n",
    "template = \"Tell me something about {topic}.\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=template,\n",
    ")\n",
    "prompt.format(topic=\"dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6b9e9-2a60-4adb-a988-3f0b2de29ac5",
   "metadata": {},
   "source": [
    "# Chain Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84043db-8886-41be-9fe1-be91e3cc4ddf",
   "metadata": {},
   "source": [
    "Chain Prompting refers to the practice of chaining consecutive prompts, where the output of a previous prompt becomes the input of the successive prompt.\n",
    "\n",
    "To use chain prompting with LangChain, you could:\n",
    "\n",
    "- Extract relevant information from the generated response.\n",
    "- Use the extracted information to create a new prompt that builds upon the previous response.\n",
    "- Repeat steps as needed until the desired output is achieved.\n",
    "\n",
    "**PromptTemplate** class makes constructing prompts with dynamic inputs easier. This is useful when creating a prompt chain that depends on previous answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11856b8e-9b21-4ac6-b4c9-d00264ebe1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientist: Albert Einstein\n",
      "\n",
      "Fact: Albert Einstein's theory of general relativity is a theory of gravity that explains how massive objects interact with each other and how they affect the fabric of space and time. According to this theory, gravity is not a force between masses, but rather a curvature of space and time caused by the presence of massive objects. This curvature is what causes objects to move towards each other, giving the appearance of a gravitational force. General relativity also predicts the phenomenon of gravitational time dilation, where time moves slower in the presence of strong gravitational fields. This theory has been extensively tested and has been shown to accurately predict the behavior of objects in the universe, including the bending of light by massive objects and the existence of black holes. It is considered one of the most important and influential theories in modern physics.\n"
     ]
    }
   ],
   "source": [
    "# prompt 1\n",
    "template_question = \"\"\"What is the anme of the famous scientist who developed the theory of general relativity?\n",
    "Answer:\"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(input_variabls=[], template = template_question)\n",
    "\n",
    "# prompt 2\n",
    "\n",
    "template_fact = \"\"\"Provide a brief description of {scientist}'s thoery of general relativity.\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "fact_prompt = PromptTemplate(input_variables=['scientist'], template=template_fact)\n",
    "\n",
    "# create llmchain for the first prompt\n",
    "\n",
    "chain_question = LLMChain(llm=llm, prompt=question_prompt)\n",
    "\n",
    "# running with empty input\n",
    "\n",
    "response_question = chain_question.run({})\n",
    "\n",
    "# extracting name from response\n",
    "scientist = response_question.strip()\n",
    "\n",
    "# create llmchain for the 2nd prompt\n",
    "\n",
    "chain_fact = LLMChain(llm=llm, prompt = fact_prompt)\n",
    "\n",
    "# Input data for the second prompt\n",
    "input_data = {\"scientist\": scientist}\n",
    "\n",
    "# Run the LLMChain for the second prompt\n",
    "response_fact = chain_fact.run(input_data)\n",
    "\n",
    "print(\"Scientist:\", scientist)\n",
    "print()\n",
    "print(\"Fact:\", response_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdadd2d-892f-4ecb-a23e-e057fb5e3ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

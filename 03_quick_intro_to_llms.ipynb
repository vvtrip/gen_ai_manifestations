{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cd86d6a-bb99-49be-9dfc-cb7454a5c6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv('file.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f30e4-2473-47f0-b825-1d280d87ead7",
   "metadata": {},
   "source": [
    "# Tracking token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51ce322d-950c-49b2-99c8-dd8d0ab6fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 40\n",
      "\tPrompt Tokens: 4\n",
      "\tCompletion Tokens: 36\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $7.8e-05\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e540f94e-2e2c-40f0-8aeb-0485d1a895ab",
   "metadata": {},
   "source": [
    "# Few Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad24b1-292a-4399-ad80-76ba4b685e87",
   "metadata": {},
   "source": [
    "This approach involves using the **FewShotPromptTemplate** class, which takes in a **PromptTemplate** and a list of a few shot examples. The class formats the prompt template with a few shot examples, which helps the language model generate a better response. We can streamline this process by utilizing LangChain's FewShotPromptTemplate to structure the approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "384c67ea-0cd6-4b33-aefe-9266421592d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "            {\n",
    "            \"query\":\"What's the weather like?\",\n",
    "            \"answer\":\"It's raining cats and dogs today. better bring an umbrella!\"\n",
    "            },\n",
    "            {\n",
    "            \"query\":\"how old are you?\",\n",
    "            \"answer\":\"Age is just a number, but i am timeless.\"\n",
    "            }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "                   User: {query}\n",
    "                   AI : {answer}\n",
    "                   \"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "                    input_variables = [\"query\", \"answer\"],\n",
    "                    template = example_template\n",
    "                    )\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98b11e1a-b9c7-4a20-9b3b-ab19a164c10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42, according to Douglas Adams. But personally, I think it's to enjoy the little things and make the most out of every moment.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "llm = OpenAI(model = 'gpt-3.5-turbo-instruct', temperature=0)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "print(chain.run(\"What's the meaning of life?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4c597-e3ce-4946-b9bc-21f39d874dc4",
   "metadata": {},
   "source": [
    "# Emergent abilities, Scaling laws, and hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a1c28-c4cd-4a0d-8562-5bce6bb371fb",
   "metadata": {},
   "source": [
    "Another aspect of LLMs is their emergent abilities, which arise as a result of extensive pre-training on vast datasets. These capabilities are not explicitly programmed but emerge as the model discerns patterns within the data. LangChain models capitalize on these emergent abilities by working with various types of models, such as chat models and text embedding models. This allows LLMs to perform diverse tasks, from answering questions to generating text and offering recommendations.\n",
    "\n",
    "Lastly, scaling laws describe the relationship between model size, training data, and performance. Generally, as the model size and training data volume increase, so does the model's performance. However, this improvement is subject to diminishing returns and may not follow a linear pattern. It is essential to weigh the trade-off between model size, training data, performance, and resources spent on training when selecting and fine-tuning LLMs for specific tasks.\n",
    "\n",
    "While Large Language Models boast remarkable capabilities but are not without shortcomings, one notable limitation is the occurrence of hallucinations, in which these models produce text that appears plausible on the surface but is actually factually incorrect or unrelated to the given input. Additionally, LLMs may exhibit biases originating from their training data, resulting in outputs that can perpetuate stereotypes or generate undesired outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df420db-7dfb-48f6-a81b-23b35f7192db",
   "metadata": {},
   "source": [
    "# Examples with Easy Prompts: Text Summarization, Text Translation, and Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3a378-0247-49d1-b430-fb88278babca",
   "metadata": {},
   "source": [
    "Creating a Question-Answering Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36e36126-2bae-4db6-ba64-48ca8961597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e7429c8-dd6e-483d-99f0-34b6f1d8a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\"\n",
    "\n",
    "# Next, we will use the Hugging Face model google/flan-t5-largeCopy to answer the question. \n",
    "# The HuggingfaceHubCopy class will connect to Hugging Face’s inference API and load the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbf5a4ca-32fd-4c29-b7a0-8e2129a86128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9dd44-36e6-4a45-9282-895eb915192b",
   "metadata": {},
   "source": [
    "Asking Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c4d436b-6f81-43c4-866e-2a958a35ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris')], [Generation(text='giraffe')], [Generation(text='nitrogen')], [Generation(text='yellow')]] llm_output=None run=[RunInfo(run_id=UUID('706d225d-a9d2-4598-8e9b-5857fb1333f6')), RunInfo(run_id=UUID('c2ffcf3d-01e2-4935-a11c-5ff6af92f9a1')), RunInfo(run_id=UUID('1bb30e12-7905-4c18-ad96-0d492f1aa388')), RunInfo(run_id=UUID('a42c732b-8108-4da2-859e-2bfdc4a5ce21'))]\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "815199d5-e3ce-47e9-835f-4875b0f809c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1. The capital city of France is Paris.\\n2. The largest mammal on Earth is the blue whale.\\n3. The gas most abundant in Earth's atmosphere is nitrogen.\\n4. A ripe banana is typically yellow.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\t\"What color is a ripe banana?\\n\"\n",
    ")\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c28ae-324b-4620-ba7b-ed7ef88a811c",
   "metadata": {},
   "source": [
    "Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0737115b-4c61-4a7c-9643-f300fb86b305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LangChain offers various modules for building language model applications, which can be used individually or combined to create more complex applications, with the most basic being calling an LLM on input to generate a company name based on its product.\n"
     ]
    }
   ],
   "source": [
    "# using turbo instruct\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)\n",
    "\n",
    "text = \"\"\"LangChain provides many modules that can be used to build language model applications. \n",
    "Modules can be combined to create more complex applications, or be used individually for simple applications. \n",
    "The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. \n",
    "For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\"\"\n",
    "\n",
    "summarized_text = summarization_chain.predict(text=text)\n",
    "\n",
    "print(summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8f0dc0a-039d-4c40-8761-aaf49ad45eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain offers various modules for building language model applications, which can be used individually or combined to create more complex applications, with the basic building block being calling an LLM on input, as demonstrated in a simple example of generating a company name based on its product.\n"
     ]
    }
   ],
   "source": [
    "# using turbo\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)\n",
    "\n",
    "text = \"\"\"LangChain provides many modules that can be used to build language model applications. \n",
    "Modules can be combined to create more complex applications, or be used individually for simple applications. \n",
    "The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. \n",
    "For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\"\"\n",
    "\n",
    "summarized_text = summarization_chain.predict(text=text)\n",
    "\n",
    "print(summarized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897612df-aa0c-429a-bb45-d7f2596aa527",
   "metadata": {},
   "source": [
    "Text translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef35c96f-0b47-4ac8-a35e-e71f0424106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votre texte ici\n"
     ]
    }
   ],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)\n",
    "\n",
    "source_language = \"English\"\n",
    "target_language = \"French\"\n",
    "text = \"Your text here\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)\n",
    "\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4997bf81-8a89-49a2-afc7-765cb77bc7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv('file.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2720195d-2344-4f05-9a9a-272e3b5fdcf9",
   "metadata": {},
   "source": [
    "# Generic LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9363fb9-1a13-4f2e-9934-0a7a1f4b2a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. SoundWave\n",
      "2. AirBeats\n",
      "3. Wireless Audio Co.\n",
      "4. EarFreedom\n",
      "5. SonicWire\n",
      "6. HeadSync\n",
      "7. BlueTunes\n",
      "8. PureWireless\n",
      "9. AudioLink\n",
      "10. FreeFlow Headphones\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"product\"],\n",
    "  template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print( chain.run(\"wireless headphones\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db795985-aa0b-4d10-9cea-c4ced6a954eb",
   "metadata": {},
   "source": [
    "# Chat Models\n",
    "\n",
    "Chat Models are the most popular models in LangChain, such as ChatGPT that can incorporate GPT-3 or GPT-4 at its core. They have gained significant attention due to their ability to learn from human feedback and their user-friendly chat interface.\n",
    "\n",
    "Chat Models, such as ChatGPT, take a list of messages as input and return an AIMessage. They typically use LLMs as their underlying technology, but their APIs are more structured. Chat Models are designed to remember previous exchanges with the user in a session and use that context to generate more relevant responses. They also benefit from reinforcement learning from human feedback, which helps improve their responses. However, they may still have limitations in reasoning and require careful handling to avoid generating inappropriate content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d272865-0d61-45c0-abbd-0d2cd86fb7c3",
   "metadata": {},
   "source": [
    "**Chat Message Types**\n",
    "\n",
    "In LangChain, three main types of messages are used when interacting with chat models: SystemMessage, HumanMessage, and AIMessage.\n",
    "\n",
    "**SystemMessage:** These messages provide initial instructions, context, or data for the AI model. They set the objectives the AI should follow and can help in controlling the AI's behavior. System messages are not user inputs but rather guidelines for the AI to operate within.\n",
    "\n",
    "**HumanMessage:** These messages come from the user and represent their input to the AI model. The AI model is expected to respond to these messages. In LangChain, you can customize the human prefix (e.g., \"User\") in the conversation summary to change how the human input is represented.\n",
    "\n",
    "**AIMessage:** These messages are sent from the AI's perspective as it interacts with the human user. They represent the AI's responses to human input. Like HumanMessage, you can also customize the AI prefix (e.g., \"AI Assistant\" or \"AI\") in the conversation summary to change how the AI's responses are represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a99fa0-3f6b-44fa-b78c-9e73f475c92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime la programmation.\", response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 31, 'total_tokens': 38}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-900785f7-fd73-451e-8e59-182b0e0c4666-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "  HumanMessage,\n",
    "  SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "\tSystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "\tHumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b2b5b-42a3-425e-852e-a68c28acfab1",
   "metadata": {},
   "source": [
    "\n",
    "SystemMessage represents the messages generated by the system that wants to use the model, which could include instructions, notifications, or error messages. These messages are not generated by the human user or the AI chatbot but are instead produced by the underlying system to provide context, guidance, or status updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d0e4aa-3b9d-4517-a8c9-104e80835103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text=\"J'aime la programmation.\", generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content=\"J'aime la programmation.\", response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 31, 'total_tokens': 38}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e291a336-ae26-4d66-a769-8296cd9d3707-0'))], [ChatGeneration(text='I love programming.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='I love programming.', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 34, 'total_tokens': 38}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-38997f3e-8439-48ec-8fb4-a4442acf0e5f-0'))]] llm_output={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 65, 'total_tokens': 76}, 'model_name': 'gpt-4'} run=[RunInfo(run_id=UUID('e291a336-ae26-4d66-a769-8296cd9d3707')), RunInfo(run_id=UUID('38997f3e-8439-48ec-8fb4-a4442acf0e5f'))]\n"
     ]
    }
   ],
   "source": [
    "# Using the generate method, you can also generate completions for multiple sets of messages.\n",
    "# Each batch of messages can have its own SystemMessage and will perform independently. \n",
    "\n",
    "batch_messages = [\n",
    "  [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
    "  ],\n",
    "  [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates French to English.\"),\n",
    "    HumanMessage(content=\"Translate the following sentence: J'aime la programmation.\")\n",
    "  ],\n",
    "]\n",
    "print( chat.generate(batch_messages) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b70d0e46-5382-4fa9-b34e-f7d8cbd23785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMResult(generations=[[ChatGeneration(text=\"J'aime la programmation.\", generation_info=None, \n",
    "#                         message=AIMessage(content=\"J'aime la programmation.\", \n",
    "#                         additional_kwargs={}, example=False))], \n",
    "                       \n",
    "#                        [ChatGeneration(text='I love programming.', generation_info=None, \n",
    "#                         message=AIMessage(content='I love programming.', \n",
    "#                         additional_kwargs={}, example=False))]], \n",
    "          \n",
    "#                         llm_output={'token_usage': {'prompt_tokens': 65, 'completion_tokens': 11, 'total_tokens': 76}, 'model_name': 'gpt-4'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ccb825-47f9-47a7-bc04-c6e30dcddcc3",
   "metadata": {},
   "source": [
    "As a comparison, here's what LLM and Chat Model APIs look like in LangChain.\n",
    "\n",
    "llm_output:  {'product': 'Translate the following text from English to French: Hello, how are you?', 'text': '\\n\\nBonjour, comment allez-vous?'}\n",
    "\n",
    "chat_output:  content='Bonjour, comment ça va ?' additional_kwargs={} example=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ca8ea-3930-4bb3-ae70-84dd778bb2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
